#include "ggml.h"
#include "llama.h"

#include <thread>
#include <memory>
#include <vector>
#include <string>
#include <queue>
#include <mutex>
#include <condition_variable>
#include <chrono>
#include <functional>
#include <atomic>

struct vision_ctx;
struct audition_ctx;
struct audition_audio_f32;

// Forward declaration for C++ Token2Wav
namespace omni {
namespace flow {
class Token2WavSession;
}
}

//
// omni ctx
//
struct omni_embed {
    float * embed;
    int n_pos;
};
struct omni_embeds{
    // ğŸ”§ [é«˜æ¸…æ¨¡å¼] vision_embed æ”¹ä¸ºäºŒç»´ vector
    // vision_embed[0] = overview embed (64 tokens * hidden_size)
    // vision_embed[1..n] = slice embeds (å„ 64 tokens * hidden_size)
    std::vector<std::vector<float>> vision_embed;
    std::vector<float> audio_embed;
    int index = 0;
    int end_flag = false;
};

struct LLMThreadInfo {
    int MAX_QUEUE_SIZE;
    std::queue<omni_embeds*> queue;
    std::mutex mtx;
    std::condition_variable cv;
    std::chrono::steady_clock::time_point start;
    std::chrono::steady_clock::time_point end;

    LLMThreadInfo(int maxQueueSize) : MAX_QUEUE_SIZE(maxQueueSize) {}
};

struct T2WOut {
    std::vector<llama_token> audio_tokens;  // Audio token IDs (25 tokens per chunk)
    bool is_final = false;  // Whether this is the final chunk (turn end)
    bool is_chunk_end = false;  // Whether this is the end of a TTS chunk (flush buffer, but not final)
    int round_idx = -1;  // ğŸ”§ [ä¿®å¤ç›®å½•åŒæ­¥] è½®æ¬¡ç´¢å¼•ï¼Œç”± TTS çº¿ç¨‹è®¾ç½®ï¼ŒT2W çº¿ç¨‹ä½¿ç”¨æ­¤å€¼ç¡®å®šè¾“å‡ºç›®å½•
};

struct T2WThreadInfo {
    int MAX_QUEUE_SIZE;
    std::queue<T2WOut*> queue;
    std::mutex mtx;
    std::condition_variable cv;
    std::chrono::steady_clock::time_point start;
    std::chrono::steady_clock::time_point end;

    T2WThreadInfo(int maxQueueSize) : MAX_QUEUE_SIZE(maxQueueSize) {}
};

// Projector Semantic: 2-layer MLP (LLM hidden states -> TTS embedding)
// forward(x): relu(linear1(x)) -> linear2
// ==================== æ»‘åŠ¨çª—å£é…ç½® ====================
// ğŸ”§ [#39] åŸºäº Python stream_decoder.py çš„ DuplexWindowConfig
struct SlidingWindowConfig {
    // æ»‘çª—æ¨¡å¼: "off" / "basic" / "context"
    // - "off": ç¦ç”¨æ»‘çª—
    // - "basic": åŸºç¡€æ»‘çª—ï¼ˆæŒ‰ cache é•¿åº¦è§¦å‘ï¼‰
    // - "context": å¸¦ context çš„æ»‘çª—ï¼ˆä¿ç•™ç”Ÿæˆæ–‡æœ¬åˆ° previousï¼‰
    std::string mode = "off";
    
    // åŸºç¡€æ»‘çª—å‚æ•°
    int high_water_tokens = 4000;  // é«˜æ°´ä½çº¿ï¼šè¶…è¿‡æ­¤å€¼è§¦å‘æ»‘çª—
    int low_water_tokens = 3500;   // ä½æ°´ä½çº¿ï¼šæ»‘çª—åä¿ç•™åˆ°æ­¤å€¼
    
    // RoPE å‚æ•°
    float rope_theta = 10000.0f;   // RoPE base frequency
};

// Unit å†å²è®°å½•æ¡ç›®
struct UnitEntry {
    int unit_id = -1;              // Unit ID
    int length = 0;                // è¯¥ unit åœ¨ cache ä¸­çš„é•¿åº¦ï¼ˆtokens æ•°ï¼‰
    std::string type;              // ç±»å‹: "audio" / "video" / "omni" / "system"
    std::vector<llama_token> generated_tokens;  // ç”Ÿæˆçš„ tokens
    bool is_listen = false;        // æ˜¯å¦æ˜¯ listen çŠ¶æ€
};

struct projector_hparams {
    int32_t in_dim  = 4096;  // è¾“å…¥ç»´åº¦ (LLM hidden size)
    int32_t out_dim = 768;   // è¾“å‡ºç»´åº¦ (TTS embedding size)
};

struct projector_layer {
    struct ggml_tensor * linear1_weight = nullptr;  // [in_dim, out_dim]
    struct ggml_tensor * linear1_bias   = nullptr;  // [out_dim]
    struct ggml_tensor * linear2_weight = nullptr;  // [out_dim, out_dim]
    struct ggml_tensor * linear2_bias   = nullptr;  // [out_dim]
};

struct projector_model {
    projector_hparams hparams;
    projector_layer layer;
    
    struct ggml_context * ctx_w = nullptr;
    ggml_backend_buffer_t buf_w = nullptr;
    ggml_backend_t backend = nullptr;
    ggml_backend_buffer_type_t buf_type = nullptr;
    bool initialized = false;
};

struct omni_context {
    struct vision_ctx * ctx_vision = NULL;
    struct audition_ctx * ctx_audio = NULL;
    
    struct llama_context * ctx_llama = NULL;
    struct llama_model * model = NULL;
    struct common_sampler * ctx_sampler = NULL;
    
    // ğŸ”§ [å•åŒå·¥é€‚é…] æ˜¯å¦æ‹¥æœ‰æ¨¡å‹ï¼ˆç”¨äº omni_free æ—¶å†³å®šæ˜¯å¦é‡Šæ”¾æ¨¡å‹ï¼‰
    // true: omni_init å†…éƒ¨åŠ è½½çš„æ¨¡å‹ï¼Œomni_free æ—¶éœ€è¦é‡Šæ”¾
    // false: å¤–éƒ¨ä¼ å…¥çš„å·²æœ‰æ¨¡å‹ï¼ˆæ¨¡å‹å¤ç”¨ï¼‰ï¼Œomni_free æ—¶ä¸é‡Šæ”¾
    bool owns_model = true;

    // ğŸ”§ [Length Penalty] ç”¨äºè°ƒæ•´ EOS token çš„é‡‡æ ·æ¦‚ç‡
    // length_penalty > 1.0 ä¼šé™ä½ EOS æ¦‚ç‡ï¼Œè®©æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„è¾“å‡º
    // length_penalty < 1.0 ä¼šå¢åŠ  EOS æ¦‚ç‡ï¼Œè®©æ¨¡å‹æ›´æ—©ç»“æŸ
    float length_penalty = 1.0f;

    struct llama_context * ctx_tts_llama = NULL;
    struct llama_model * model_tts = NULL;
    struct common_sampler * ctx_tts_sampler = NULL;
    
    // struct TTSContext * ctx_tts = NULL;
    struct vocal_ctx * vocal = NULL;
    std::shared_ptr<std::vector<float>> spk_embeds;
    std::vector<float> audio_emb;
    std::vector<float> omni_emb;    
    int output_audio_round_per_text[5] = {16, 8, 4, 2, 2};
    int output_audio_chunk_size[5] = {5, 10, 20, 40, 40};
    
    struct omni_output *omni_output = NULL;
    int n_past = 0;
    int n_keep = 0;
    
    // ==================== è½®æ¬¡è¾¹ç•Œç®¡ç†ï¼ˆç”¨äºæ™ºèƒ½æ»‘åŠ¨çª—å£ï¼‰ ====================
    // æ¯è½®å¯¹è¯å¼€å§‹æ—¶çš„ n_past ä½ç½®
    // round_start_positions[i] è¡¨ç¤ºç¬¬ i è½®å¼€å§‹çš„ n_past ä½ç½®
    // ç¬¬ i è½®çš„èŒƒå›´æ˜¯ [round_start_positions[i], round_start_positions[i+1])
    // æœ€åä¸€è½®çš„ç»“æŸä½ç½®æ˜¯å½“å‰ n_past
    std::vector<int> round_start_positions;
    
    // æ»‘åŠ¨çª—å£ä¿ç•™çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¸åŒ…æ‹¬ n_keepï¼‰
    // è®¾ç½®ä¸º 0 è¡¨ç¤ºä½¿ç”¨æ—§çš„æŒ‰æ¯”ä¾‹åˆ é™¤ç­–ç•¥
    int max_preserved_context = 2048;
    
    // ==================== æ»‘åŠ¨çª—å£çŠ¶æ€ (#39) ====================
    SlidingWindowConfig sliding_window_config;
    
    // Unit å†å²ç®¡ç†ï¼ˆç”¨äºæŒ‰ unit ç²’åº¦åˆ é™¤ï¼‰
    std::vector<UnitEntry> unit_history;
    int next_unit_id = 0;
    int pending_unit_id = -1;           // å½“å‰æ­£åœ¨å¤„ç†çš„ unit ID
    int pending_unit_start_cache_len = 0;  // pending unit å¼€å§‹æ—¶çš„ cache é•¿åº¦
    
    // System prompt ä¿æŠ¤é•¿åº¦ï¼ˆè¿™éƒ¨åˆ†æ°¸è¿œä¸ä¼šè¢«æ»‘çª—åˆ é™¤ï¼‰
    int system_preserve_length = 0;
    
    // RoPE ä½ç½®åç§»ï¼ˆç”¨äº RoPE ä½ç½®é‡å¯¹é½åçš„ position_ids è®¡ç®—ï¼‰
    int position_offset = 0;
    
    // æ»‘çª—ç»Ÿè®¡
    int sliding_event_count = 0;    // æ»‘çª—è§¦å‘æ¬¡æ•°
    int total_dropped_tokens = 0;   // æ€»å…±ä¸¢å¼ƒçš„ token æ•°
    int total_dropped_units = 0;    // æ€»å…±ä¸¢å¼ƒçš„ unit æ•°
    
    bool async = false;
    std::thread llm_thread;
    std::thread tts_thread;
    std::thread t2w_thread;
    struct LLMThreadInfo *llm_thread_info = NULL;
    struct TTSThreadInfo *tts_thread_info = NULL;
    struct T2WThreadInfo *t2w_thread_info = NULL;
    
    volatile bool need_speek = false;
    volatile bool speek_done = true;
    
    // é¢„çƒ­æ ‡å¿—ï¼šç¬¬ä¸€è½®å¯¹è¯è§†ä¸ºé¢„çƒ­ï¼ˆä¾‹å¦‚éŸ³è‰²å…‹éš†å‚è€ƒéŸ³é¢‘ï¼‰ï¼Œå®Œæˆåè®¾ä¸º true
    std::atomic<bool> warmup_done{false};
    
    // ==================== åŒå·¥æ¨¡å¼çŠ¶æ€ ====================
    // å½“å‰è½®æ¬¡æ˜¯å¦å·²ç»“æŸï¼ˆç”¨äºå†³ç­–æ˜¯å¦å…è®¸åˆ‡æ¢åˆ° listen çŠ¶æ€ï¼‰
    // Python: self.current_turn_ended
    bool current_turn_ended = true;
    
    // æ‰“æ–­äº‹ä»¶æ ‡å¿—
    // break_event: æ‰“æ–­å½“å‰ç”Ÿæˆï¼Œä½†ä¿æŒä¼šè¯æ´»è·ƒï¼ˆç”¨äºåŒå·¥æ¨¡å¼çš„ç”¨æˆ·æ‰“æ–­ï¼‰
    //              æ‰“æ–­åå¯ç»§ç»­è°ƒç”¨ prefill/decode
    std::atomic<bool> break_event{false};
    
    // session_stop_event: ç»ˆæ­¢æ•´ä¸ªä¼šè¯ï¼ˆé¢„ç•™ï¼Œç›®å‰æœªä½¿ç”¨ï¼‰
    //                     ç”¨äºå½»åº•å…³é—­å½“å‰ä¼šè¯ï¼Œéœ€è¦é‡æ–° omni_init
    std::atomic<bool> session_stop_event{false};
    
    // ğŸ”§ [åŒå·¥æ¨¡å¼] è®°å½•å½“å‰ decode æ˜¯å¦ä»¥ <|listen|> ç»“æŸ
    // å¦‚æœæ˜¯ï¼Œåˆ™ä¸æ¸…ç† KV cacheï¼Œè®©ä¸‹ä¸€ä¸ªéŸ³é¢‘ç‰‡æ®µå¯ä»¥ç´¯ç§¯ä¸Šä¸‹æ–‡
    std::atomic<bool> ended_with_listen{false};
    
    // ğŸ”§ [ä¸ Python å¯¹é½] LLM ç”Ÿæˆç»“æŸæ ‡å¿—
    // å½“ LLM æ£€æµ‹åˆ° end token æ—¶è®¾ç½®ä¸º true
    // TTS çº¿ç¨‹æ£€æŸ¥æ­¤æ ‡å¿—æ¥å†³å®šæ˜¯å¦æ·»åŠ  text_eos_embed
    std::atomic<bool> llm_generation_done{false};
    
    // ==================== åŒå·¥æ¨¡å¼å‚æ•° ====================
    // æ¯ä¸ª chunk æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆç”¨äºé™åˆ¶å•æ¬¡ speak é•¿åº¦ï¼Œä¾¿äºåŠæ—¶å“åº”æ‰“æ–­ï¼‰
    // è®¾ç½®ä¸º 0 è¡¨ç¤ºæ— é™åˆ¶
    int max_new_speak_tokens_per_chunk = 26;
    
    // listen_prob_scale: è°ƒæ•´ <|listen|> token çš„é‡‡æ ·æ¦‚ç‡
    // 1.0: Python é»˜è®¤
    float listen_prob_scale = 1.0f;
    
    // æ˜¯å¦å¯ç”¨åŒå·¥æ¨¡å¼
    // simplex: å•å·¥æ¨¡å¼ï¼Œç”¨æˆ·è¯´å®Œåæ¨¡å‹å›å¤ï¼Œå›å¤å®Œç”¨æˆ·å†è¯´
    // duplex: åŒå·¥æ¨¡å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä»»æ„æ—¶åˆ»å†³å®šå¬/è¯´åˆ‡æ¢
    bool duplex_mode = false;
    
    // ç³»ç»Ÿ prompt æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆé˜²æ­¢ stream_prefill index=0 è¢«é‡å¤è°ƒç”¨å¯¼è‡´ prompt é‡å¤ï¼‰
    bool system_prompt_initialized = false;
    
    class AudioInputManager * audio_input_manager = NULL;
    
    // models path and other configs
    struct common_params * params = NULL;
    
    // å½“å‰æ˜¯ä»¥ã€Œè¯­éŸ³é€šè¯ã€è¿˜æ˜¯ã€Œè§†é¢‘é€šè¯ã€æ¨¡å¼è¿›å…¥çš„ï¼Œ0 = è¯­éŸ³ï¼Œ1 = è§†é¢‘ï¼›
    int media_type = 0;
    int use_tts = false;
    std::string tts_bin_dir = "";
    std::string ref_audio_path = "";  // å‚è€ƒéŸ³é¢‘è·¯å¾„ï¼ˆç”¨äºéŸ³è‰²å…‹éš†ï¼‰
    
    // ğŸ”§ [é«˜æ¸…/é«˜åˆ·æ¨¡å¼] 
    // high_image: é«˜æ¸…æ¨¡å¼ï¼Œmax_slice_nums è®¾ç½®ä¸º 2ï¼Œvision å¯ä»¥çœ‹åˆ°æ›´å¤šç»†èŠ‚
    // high_refresh: é«˜åˆ·æ¨¡å¼ï¼Œ1ç§’5å¸§ï¼Œç¬¬1å¸§ä½œä¸ºä¸»å›¾ï¼Œå4å¸§stackåˆå¹¶æˆä¸€å¼ å›¾
    //               æ³¨æ„ï¼šstack å¤„ç†åœ¨ Python server å±‚å®ç°ï¼ŒC++ åªæ˜¯æ ‡è®°
    bool high_image = false;
    bool high_refresh = false;
    
    // ğŸ”§ [å¤šå®ä¾‹æ”¯æŒ] å¯é…ç½®çš„è¾“å‡ºç›®å½•ï¼Œé¿å…å¤šä¸ªæœåŠ¡å®ä¾‹å†²çª
    std::string base_output_dir = "./tools/omni/output";
    
    // æ¯æ¬¡ä¼šè¯ï¼Œæ˜¯å¦æ¸…é™¤ kv cacheï¼ˆé»˜è®¤å¼€å¯è‡ªåŠ¨æ¸…ç† kv cacheï¼‰
    bool clean_kvcache = true;
    
    std::string omni_voice_clone_prompt = "";
    std::string omni_assistant_prompt = "";
    std::string audio_voice_clone_prompt = "";
    std::string audio_assistant_prompt = "";
    
    // è¯­è¨€è®¾ç½® (ç”¨äº prompt ç”Ÿæˆ)
    std::string language = "zh";

    // text streaming queue for server
    std::mutex text_mtx;
    std::condition_variable text_cv;
    std::deque<std::string> text_queue;
    bool text_streaming = false;
    bool text_done_flag = false;

    // llama inference mutex - ä¿æŠ¤ ctx_llama çš„æ¨ç†æ“ä½œ
    std::mutex llama_mtx;
    
    // TTS weights loaded from GGUF file
    // emb_code: (num_audio_tokens=6562, hidden_size=768) - for converting audio token IDs to embeddings
    float * emb_code_weight = nullptr;
    int emb_code_vocab_size = 0;  // num_audio_tokens = 6562
    int emb_code_hidden_size = 0; // hidden_size = 768
    bool emb_code_stored_as_transposed = false; // true if stored as [hidden_size, num_audio_tokens] = [768, 6562]
    
    // emb_text: (vocab_size=152064, hidden_size=768)
    float * emb_text_weight = nullptr;
    int emb_text_vocab_size = 0;
    int emb_text_hidden_size = 0;
    
    // projector_semantic: two-layer MLP (4096 -> 768 -> 768)
    // Legacy float* weights (kept for backward compatibility)
    float * projector_semantic_linear1_weight = nullptr;  // (4096, 768)
    float * projector_semantic_linear1_bias = nullptr;   // (768,)
    float * projector_semantic_linear2_weight = nullptr; // (768, 768)
    float * projector_semantic_linear2_bias = nullptr;  // (768,)
    int projector_semantic_input_dim = 0;  // 4096
    int projector_semantic_output_dim = 0;  // 768
    
    // New ggml-based projector model (ç²¾åº¦éªŒè¯ç‰ˆæœ¬)
    struct projector_model projector;
    
    // head_code: Linear layer (hidden_size=768 -> num_audio_tokens=6562)
    // Note: num_vq=1, so we only need one head_code layer
    float * head_code_weight = nullptr;  // (768, 6562) - stored as (hidden_size, num_audio_tokens)
    int head_code_hidden_size = 0;  // 768
    int head_code_num_audio_tokens = 0;  // 6562
    
    // TTS condition embeddings (for first audio token re-forward)
    // Used to store the condition embeddings so we can re-forward them for the first audio token
    // This ensures KV cache state matches Python's behavior (past_key_values=None on first forward)
    std::vector<float> tts_condition_embeddings;  // Condition embeddings (n_tokens * n_embd)
    int tts_condition_length = 0;  // Number of tokens in condition
    int tts_condition_n_embd = 0;  // Embedding dimension (768)
    bool tts_condition_saved = false;  // Whether condition has been saved
    
    // ğŸ”§ TTS KV cache ç´¯è®¡ä½ç½®ï¼ˆç”¨äºä¿æŒè·¨ chunk çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼‰
    // Python TTSStreamingGenerator ä½¿ç”¨ text_start_pos æ¥è·Ÿè¸ªä½ç½®
    int tts_n_past_accumulated = 0;
    
    // ğŸ”§ [å…³é”®ä¿®å¤] TTS å·²ç”Ÿæˆçš„æ‰€æœ‰ audio tokensï¼ˆè·¨ chunk ç´¯ç§¯ï¼‰
    // Python: self.all_generated_tokens æ˜¯ç±»æˆå‘˜å˜é‡ï¼Œè·¨ chunk æŒç»­ç´¯ç§¯
    // ç”¨äºï¼š1. RAS é‡å¤æ£€æµ‹ï¼ˆéœ€è¦å®Œæ•´å†å²ï¼‰2. æ­£ç¡®åˆ¤æ–­ audio_bosï¼ˆåªæœ‰ç¬¬ä¸€ä¸ª token æ‰æ˜¯ï¼‰
    std::vector<llama_token> tts_all_generated_tokens;
    
    // ğŸ”§ [ä¸ Python å¯¹é½] TTS audio token bufferï¼ˆè·¨ text chunk ç´¯ç§¯ï¼‰
    // Python: self._token_buffer æ˜¯ç±»æˆå‘˜å˜é‡ï¼Œç”¨äºç´¯ç§¯ audio token
    // åªæœ‰æ»¡è¶³ chunk_size (25) æ‰ä¼š yieldï¼Œä¸è¶³çš„ä¿ç•™åˆ°ä¸‹ä¸€ä¸ª text chunk
    std::vector<int32_t> tts_token_buffer;
    
    // Timestamp for stream_decode start (used for WAV file naming)
    std::chrono::high_resolution_clock::time_point stream_decode_start_time;
    
    // C++ Token2Wav session for audio synthesis
    std::unique_ptr<omni::flow::Token2WavSession> token2wav_session;
    bool token2wav_initialized = false;
    std::string token2wav_model_dir;  // Directory containing token2wav GGUF models
    
    // ğŸ”§ [Python Token2Wav] ä½¿ç”¨ Python stepaudio2 åº“å®ç°çš„ Token2Wav
    // è®¾ç½®ä¸º true æ—¶ä½¿ç”¨ Python å®ç°ï¼ˆç²¾åº¦æ›´é«˜ï¼‰ï¼Œfalse æ—¶ä½¿ç”¨ C++ å®ç°
    // macOS ä¸Šé»˜è®¤ä½¿ç”¨ C++ å®ç°ï¼ˆæ—  CUDAï¼‰
    bool use_python_token2wav = false;
    std::string python_t2w_script_dir;  // Python Token2Wav è„šæœ¬ç›®å½•
    std::string python_t2w_model_dir;   // Python Token2Wav æ¨¡å‹ç›®å½•
    
    // Python Token2Wav æœåŠ¡è¿›ç¨‹ (é€šè¿‡ popen å¯åŠ¨)
    FILE* python_t2w_stdin = nullptr;   // å†™å…¥å‘½ä»¤
    FILE* python_t2w_stdout = nullptr;  // è¯»å–å“åº”
    pid_t python_t2w_pid = -1;          // è¿›ç¨‹ ID
    bool python_t2w_initialized = false;
    std::string python_t2w_gpu_id;      // GPU ID (å¦‚ "0", "1")
    
    // ğŸ”§ Python T2W ç‹¬ç«‹ GPU é…ç½®
    // C++ LLM+TTS å ç”¨çº¦ 22GBï¼ŒPython T2W å ç”¨çº¦ 3.3GB
    // å•å¡ 24GB æ”¾ä¸ä¸‹ï¼Œéœ€è¦ä½¿ç”¨ç‹¬ç«‹ GPU
    // è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä½¿ç”¨ä¸ C++ ç›¸åŒçš„ GPU
    std::string python_t2w_dedicated_gpu = "";  // ç‹¬ç«‹ GPU IDï¼Œå¦‚ "1"
    
    // Token2Wav sliding window buffer (è·¨ chunk ä¿æŒçŠ¶æ€)
    // Python é€»è¾‘: buffer åˆå§‹å¡«å…… 3 ä¸ªé™éŸ³ token (4218)
    // æ¯æ¬¡å– 28 ä¸ª tokens (25 main + 3 lookahead)ï¼Œå¤„ç†åç§»åŠ¨ 25 ä¸ªï¼Œä¿ç•™ 3 ä¸ªé‡å 
    std::vector<int32_t> token2wav_buffer;
    int token2wav_wav_idx = 0;  // è¾“å‡º WAV æ–‡ä»¶è®¡æ•°å™¨
    int wav_turn_base = 0;      // æ¯è½®å¯¹è¯ç»“æŸæ—¶ +1000ï¼Œç”¨äºåŒºåˆ†ä¸åŒè½®æ¬¡çš„ WAV æ–‡ä»¶
    
    // ğŸ”§ [å•å·¥æ¨¡å¼] å½“å‰è½®æ¬¡ç´¢å¼•ï¼ˆç”¨äºåˆ›å»º round_000ã€round_001 ç­‰å­ç›®å½•ï¼‰
    int simplex_round_idx = 0;
    
    // ==================== ç‰¹æ®Š Token ID ====================
    // åœ¨ omni_init æ—¶ä»è¯è¡¨æŸ¥æ‰¾å¹¶ç¼“å­˜
    llama_token special_token_speak = -1;        // <|speak|>: æ¨¡å‹å¼€å§‹è¯´è¯
    llama_token special_token_listen = -1;       // <|listen|>: æ¨¡å‹å¼€å§‹å¬ï¼ˆåŒå·¥ï¼‰
    llama_token special_token_chunk_eos = -1;    // <|chunk_eos|>: è¯­ä¹‰ chunk ç»“æŸ
    llama_token special_token_chunk_tts_eos = -1;// <|chunk_tts_eos|>: TTS chunk ç»“æŸ
    llama_token special_token_turn_eos = -1;     // <|turn_eos|>: è½®æ¬¡ç»“æŸ
    llama_token special_token_tts_eos = -1;      // <|tts_eos|>: æ—§ç‰ˆ TTS ç»“æŸ
    llama_token special_token_eos = -1;          // </s>: åºåˆ—ç»“æŸ
    llama_token tts_bos_token_id = -1;           // <|tts_bos|>: TTS å¼€å§‹ï¼ˆç”¨äºåŒå·¥å¼ºåˆ¶ç»§ç»­è¯´è¯ï¼‰
    llama_token special_token_unit_end = -1;     // </unit>: unit ç»“æŸæ ‡è®°ï¼ˆåŒå·¥ chunk è¾¹ç•Œï¼‰
    llama_token special_token_tts_pad = -1;      // <|tts_pad|>: TTS å¡«å……ï¼ˆåŒå·¥æ¨¡å¼ä¸‹ç¦æ­¢é‡‡æ ·ï¼‰
};

//
// omni embed
//
bool prefill_with_emb(struct omni_context * ctx_omni, struct common_params * params, float* embed, int n_pos, int n_batch, int* n_past);
bool prefill_emb_with_hidden(struct omni_context * ctx_omni, struct common_params * params, float* embed, int n_pos, int n_batch, int* n_past, float *& hidden_states);
bool omni_eval_embed(struct llama_context * ctx_llama, const struct omni_embed * embed, int n_batch, int * n_past);
void omni_embed_free(struct omni_embed * embed);
struct omni_embed * omni_image_embed_make_with_bytes(struct vision_ctx * ctx_vision, int n_threads, const unsigned char * image_bytes, int image_bytes_length);
struct omni_embed * omni_image_embed_make_with_filename(struct vision_ctx * ctx_vision, int n_threads, std::string image_path);
struct omni_embed * omni_audio_embed_make_with_bytes(struct audition_ctx * ctx_audition, int n_threads, audition_audio_f32 * audio);
struct omni_embed * omni_audio_embed_make_with_filename(struct audition_ctx * ctx_audition, int n_threads, std::string audio_path);

//
// omni main
//
struct omni_context * omni_init(struct common_params * params, int media_type, bool use_tts, std::string tts_bin_dir,
                                int tts_gpu_layers = -1, const std::string & token2wav_device = "gpu:0",
                                bool duplex_mode = false,
                                llama_model * existing_model = nullptr, llama_context * existing_ctx = nullptr,
                                const std::string & base_output_dir = "./tools/omni/output");

void omni_free(struct omni_context * ctx_omni);

// ANE/CoreML warmup â€” call once after omni_init to pre-load models into NPU
void omni_warmup_ane(struct omni_context * ctx_omni);

// åœæ­¢æ‰€æœ‰çº¿ç¨‹ï¼ˆåœ¨ join ä¹‹å‰è°ƒç”¨ï¼‰
void omni_stop_threads(struct omni_context * ctx_omni);

bool stream_prefill(struct omni_context * ctx_omni,
                            std::string aud_fname,
                            std::string img_fname = "",
                            int index = 0,
                            int max_slice_nums = -1);  // -1 è¡¨ç¤ºä½¿ç”¨å…¨å±€è®¾ç½®ï¼Œ>=1 è¡¨ç¤ºæœ¬æ¬¡ prefill çš„ slice æ•°é‡

bool stream_decode(struct omni_context * ctx_omni,
                        std::string debug_dir,
                        int round_idx = -1);  // round_idx: ç”±è°ƒç”¨æ–¹æŒ‡å®šçš„è½®æ¬¡ç´¢å¼•ï¼Œ-1 è¡¨ç¤ºä½¿ç”¨å†…éƒ¨è®¡æ•°

bool stop_speek(struct omni_context * ctx_omni);

bool clean_kvcache(struct omni_context * ctx_omni);

// TTS æ¨ç†å‡½æ•°å£°æ˜ï¼ˆç”¨äº test_tts_inference.cppï¼‰
bool load_tts_weights_from_gguf(struct omni_context * ctx_omni, const char * tts_model_path);
bool prefill_with_emb_tts(struct omni_context* ctx_omni, common_params* params, float* embed, int n_pos, int n_batch, int* n_past_tts);
// sample_tts_token å‚æ•°è¯´æ˜ï¼š
// - all_generated_tokens: è·¨ chunk ç´¯ç§¯çš„æ‰€æœ‰ tokensï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯æ•´ä¸ªè¿‡ç¨‹çš„ç¬¬ä¸€ä¸ª tokenï¼Œå³ re-forward conditionï¼‰
// - chunk_generated_tokens: å½“å‰ chunk å†…å·²ç”Ÿæˆçš„ tokensï¼ˆç”¨äº repetition penaltyï¼Œä¸ Python generate_chunk å¯¹é½ï¼‰
// - token_index_in_chunk: å½“å‰ chunk å†…çš„ token ç´¢å¼•ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦è·³è¿‡ sampling processorsï¼‰
// - force_no_eos: æ˜¯å¦å¼ºåˆ¶é˜»æ­¢ EOS token è¢«é‡‡æ ·ï¼ˆç”¨äº min_new_tokens é€»è¾‘ï¼Œä¸ Python generate_chunk å¯¹é½ï¼‰
llama_token sample_tts_token(struct common_sampler * smpl, struct omni_context * ctx_omni, common_params* params, int * n_past_tts, const std::vector<llama_token> * all_generated_tokens = nullptr, const std::vector<llama_token> * chunk_generated_tokens = nullptr, int token_index_in_chunk = 0, bool force_no_eos = false);

// Projector å‡½æ•°å£°æ˜ï¼ˆç²¾åº¦éªŒè¯ç‰ˆæœ¬ï¼‰
bool projector_init(projector_model & model, const std::string & fname, bool use_cuda);
void projector_free(projector_model & model);
std::vector<float> projector_forward(projector_model & model, const float * input_data, int n_tokens);

// ==================== æ»‘åŠ¨çª—å£å‡½æ•°å£°æ˜ (#39) ====================
// Unit ç®¡ç†
int sliding_window_register_unit_start(struct omni_context * ctx_omni);
void sliding_window_register_unit_end(struct omni_context * ctx_omni, const std::string & input_type, 
                                      const std::vector<llama_token> & generated_tokens = {}, bool is_listen = false);
void sliding_window_register_system_prompt(struct omni_context * ctx_omni);

// æ»‘çª—æ‰§è¡Œ
bool sliding_window_enforce(struct omni_context * ctx_omni);
bool sliding_window_drop_tokens_from_cache(struct omni_context * ctx_omni, int length);
void sliding_window_reset(struct omni_context * ctx_omni);

// ==================== é«˜æ¸…æ¨¡å¼å‡½æ•°å£°æ˜ ====================
// è®¾ç½® vision max_slice_nums è¦†ç›–å€¼ï¼Œç”¨äºé«˜æ¸…æ¨¡å¼
void vision_set_max_slice_nums(struct vision_ctx * ctx_vision, int max_slice_nums);