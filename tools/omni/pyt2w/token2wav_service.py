#!/usr/bin/env python3
"""
Token2Wav æœåŠ¡è¿›ç¨‹ - ç”¨äº C++ è°ƒç”¨ Python çš„ stepaudio2 Token2wav

åè®®ï¼šé€šè¿‡ stdin æ¥æ”¶ JSON å‘½ä»¤ï¼Œé€šè¿‡ stdout è¿”å› JSON å“åº”

å‘½ä»¤æ ¼å¼:
- init: {"cmd": "init", "model_dir": "/path/to/model", "device": "cuda:0", "float16": true, "n_timesteps": 5}
- set_ref_audio: {"cmd": "set_ref_audio", "ref_audio_path": "/path/to/ref.wav"}
- process: {"cmd": "process", "tokens": [1,2,3,...], "last_chunk": false, "output_path": "/path/to/output.wav"}
- reset: {"cmd": "reset"}
- quit: {"cmd": "quit"}

å“åº”æ ¼å¼:
- æˆåŠŸ: {"status": "ok", "message": "...", ...}
- å¤±è´¥: {"status": "error", "message": "..."}

æ³¨æ„: CUDA_VISIBLE_DEVICES å¿…é¡»åœ¨å¯åŠ¨è„šæœ¬å‰é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼
"""

import os
import sys

# ğŸ”§ é‡å®šå‘åº“çš„ stdout è¾“å‡ºåˆ° stderrï¼Œé¿å…å¹²æ‰° JSON åè®®
# ä¿å­˜åŸå§‹ stdout ç”¨äº JSON é€šä¿¡
_original_stdout = sys.stdout
_original_stderr = sys.stderr

# åˆ›å»ºä¸€ä¸ªæ–°çš„ stderr-like å¯¹è±¡æ¥æ•è·åº“çš„æ‰“å°è¾“å‡º
class StderrRedirector:
    def write(self, text):
        _original_stderr.write(text)
    def flush(self):
        _original_stderr.flush()

# åœ¨å¯¼å…¥å…¶ä»–åº“ä¹‹å‰ï¼Œä¸´æ—¶é‡å®šå‘ stdout åˆ° stderr
sys.stdout = StderrRedirector()

import json
import time
import traceback
import numpy as np

# ç¦ç”¨ tokenizers çš„å¹¶è¡Œå¤„ç†è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ¢å¤åŸå§‹ stdout ç”¨äº JSON é€šä¿¡
sys.stdout = _original_stdout

def log(msg):
    """è¾“å‡ºæ—¥å¿—åˆ° stderrï¼ˆä¸å½±å“ stdout çš„ JSON åè®®ï¼‰"""
    print(f"[T2W-PY] {msg}", file=sys.stderr, flush=True)


class Token2WavService:
    def __init__(self):
        self.token2wav = None
        self.stream_cache = None
        self.hift_cache = None
        self.ref_audio_path = None
        self.initialized = False
        self.device = "cuda:0"
        
    def init(self, model_dir: str, device: str = "cuda:0", float16: bool = True, n_timesteps: int = 5):
        """åˆå§‹åŒ– Token2Wav æ¨¡å‹"""
        try:
            # ğŸ”§ åœ¨å¯¼å…¥å¯èƒ½æœ‰è¾“å‡ºçš„åº“ä¹‹å‰ï¼Œä¸´æ—¶é‡å®šå‘ stdout åˆ° stderr
            import sys
            original_stdout = sys.stdout
            sys.stdout = sys.stderr
            
            try:
                # ğŸ”§ è®¾å¤‡æ ¼å¼è½¬æ¢: "gpu:0" -> "cuda:0", "gpu" -> "cuda:0"
                if device.startswith("gpu"):
                    if ":" in device:
                        gpu_id = device.split(":")[1]
                        device = f"cuda:{gpu_id}"
                    else:
                        device = "cuda:0"
                
                self.device = device
                
                # ğŸ”§ æ³¨æ„: CUDA_VISIBLE_DEVICES å¿…é¡»åœ¨ C++ fork å­è¿›ç¨‹æ—¶è®¾ç½®
                # è¿™é‡Œçš„è®¾ç½®å·²ç»å¤ªæ™šäº†ï¼ˆtorch å¯èƒ½å·²è¢«å¯¼å…¥ï¼‰ï¼Œä»…ä½œä¸ºæ—¥å¿—è®°å½•
                cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "not set")
                log(f"åˆå§‹åŒ– Token2Wav: model_dir={model_dir}, device={device}, float16={float16}, n_timesteps={n_timesteps}")
                log(f"CUDA_VISIBLE_DEVICES={cuda_visible}")
                
                import torch

                # macOS/CPU fallback: some third-party code may hard-call .cuda().
                if not torch.cuda.is_available():
                    log("âš ï¸ CUDA not available. Monkey-patching .cuda() to use MPS or CPU.")

                    target_device = "mps" if torch.backends.mps.is_available() else "cpu"
                    log(f"Using fallback device: {target_device}")

                    def custom_cuda(self, device=None):
                        if target_device == "mps":
                            if isinstance(self, torch.Tensor) and self.dtype == torch.float64:
                                return self.float().to(target_device)
                        return self.to(target_device)

                    torch.nn.Module.cuda = custom_cuda
                    torch.Tensor.cuda = custom_cuda

                    device = target_device
                    self.device = target_device

                    # Workaround for environments where torchaudio backend is unavailable.
                    try:
                        import torchaudio
                        import soundfile

                        def replacement_load(uri, **kwargs):
                            data, sr = soundfile.read(uri)
                            tensor = torch.from_numpy(data)

                            if tensor.ndim == 1:
                                tensor = tensor.unsqueeze(1)

                            # soundfile: [time, channels] -> torchaudio style: [channels, time]
                            tensor = tensor.transpose(0, 1)

                            if tensor.dtype != torch.float32:
                                tensor = tensor.float()

                            return tensor, sr

                        torchaudio.load = replacement_load
                        log("Monkey-patched torchaudio.load to use soundfile")
                    except ImportError:
                        log("Failed to patch torchaudio.load: soundfile not installed?")
                    except Exception as e:
                        log(f"Failed to patch torchaudio.load: {e}")

                log(f"PyTorch CUDA available: {torch.cuda.is_available()}, device_count: {torch.cuda.device_count()}")
                
                from stepaudio2 import Token2wav
                self.token2wav = Token2wav(model_dir, float16=float16, n_timesteps=n_timesteps)
                
                # ğŸ”§ ä¿®å¤ float16 æ¨¡å¼ä¸‹çš„ dtype bug
                # stepaudio2 åº“çš„ setup_cache æ–¹æ³•åœ¨ float16 æ¨¡å¼ä¸‹ä¼šå‡ºç°è¾“å…¥æ˜¯ float32 ä½†æƒé‡æ˜¯ float16 çš„é—®é¢˜
                if float16:
                    original_setup_cache = self.token2wav.flow.setup_cache
                    
                    @torch.inference_mode()
                    def patched_setup_cache(prompt_speech_tokens, prompt_mels, spk, n_timesteps):
                        # å°†è¾“å…¥è½¬æ¢ä¸º float16
                        if prompt_mels.dtype != torch.float16:
                            prompt_mels = prompt_mels.half()
                        if spk.dtype != torch.float16:
                            spk = spk.half()
                        return original_setup_cache(prompt_speech_tokens, prompt_mels, spk, n_timesteps)
                    
                    self.token2wav.flow.setup_cache = patched_setup_cache
                    log("å·²åº”ç”¨ float16 dtype ä¿®å¤è¡¥ä¸")
                
                self.initialized = True
                
                log("Token2Wav åˆå§‹åŒ–æˆåŠŸ")
            finally:
                # æ¢å¤åŸå§‹ stdout
                sys.stdout = original_stdout
            
            return {"status": "ok", "message": "Token2Wav initialized"}
            
        except Exception as e:
            log(f"Token2Wav åˆå§‹åŒ–å¤±è´¥: {e}")
            traceback.print_exc(file=sys.stderr)
            return {"status": "error", "message": str(e)}
    
    def set_ref_audio(self, ref_audio_path: str):
        """è®¾ç½®å‚è€ƒéŸ³é¢‘ï¼Œåˆå§‹åŒ–æµå¼ç¼“å­˜"""
        if not self.initialized:
            return {"status": "error", "message": "Token2Wav not initialized"}
        
        try:
            # ğŸ”§ ä¸´æ—¶é‡å®šå‘ stdout åˆ° stderrï¼Œé¿å…åº“çš„æ‰“å°è¾“å‡ºå¹²æ‰° JSON åè®®
            import sys
            original_stdout = sys.stdout
            sys.stdout = sys.stderr
            
            try:
                import torch
                
                log(f"è®¾ç½®å‚è€ƒéŸ³é¢‘: {ref_audio_path}")
                
                if not os.path.exists(ref_audio_path):
                    return {"status": "error", "message": f"Reference audio not found: {ref_audio_path}"}
                
                self.ref_audio_path = ref_audio_path
                
                # è°ƒç”¨ set_stream_cache è®¾ç½®ç¼“å­˜
                self.stream_cache, self.hift_cache = self.token2wav.set_stream_cache(ref_audio_path)
                
                # æ·±æ‹·è´åŸºç¡€ç¼“å­˜ï¼Œç”¨äºåç»­é‡ç½®
                self.stream_cache_base = self._clone_cache(self.stream_cache)
                self.hift_cache_base = self._clone_cache(self.hift_cache)
                
                log("å‚è€ƒéŸ³é¢‘è®¾ç½®æˆåŠŸ")
                
                # ğŸ”§ Warmup: ç”¨ dummy tokens è·‘ä¸€æ¬¡æ¨ç†ï¼Œé¢„ç¼–è¯‘ CUDA kernels
                # è¿™æ ·é¦–æ¬¡çœŸæ­£æ¨ç†å°±ä¸ä¼šæœ‰å†·å¯åŠ¨å»¶è¿Ÿ
                log("å¼€å§‹ warmup (é¢„ç¼–è¯‘ CUDA kernels)...")
                warmup_start = time.time()
                
                # ä½¿ç”¨ audio_bos token (4218) ä½œä¸º dummy tokens
                dummy_tokens = [4218, 4218, 4218] + [1000] * 25  # 28 tokens
                
                # è®¾ç½®ç¼“å­˜
                self.token2wav.stream_cache = self._clone_cache(self.stream_cache_base)
                self.token2wav.hift_cache_dict = self._clone_cache(self.hift_cache_base)
                
                # è·‘ä¸€æ¬¡æ¨ç†
                _ = self.token2wav.stream(
                    generated_speech_tokens=dummy_tokens,
                    prompt_wav=ref_audio_path,
                    last_chunk=True,
                    return_waveform=True
                )
                
                # é‡ç½®ç¼“å­˜åˆ°åˆå§‹çŠ¶æ€
                self.stream_cache = self._clone_cache(self.stream_cache_base)
                self.hift_cache = self._clone_cache(self.hift_cache_base)
                
                warmup_time = time.time() - warmup_start
                log(f"warmup å®Œæˆï¼Œè€—æ—¶ {warmup_time*1000:.1f}ms")
            finally:
                # æ¢å¤åŸå§‹ stdout
                sys.stdout = original_stdout
            
            return {"status": "ok", "message": "Reference audio set"}
            
        except Exception as e:
            log(f"è®¾ç½®å‚è€ƒéŸ³é¢‘å¤±è´¥: {e}")
            traceback.print_exc(file=sys.stderr)
            return {"status": "error", "message": str(e)}
    
    def _clone_cache(self, cache):
        """æ·±æ‹·è´ç¼“å­˜"""
        import torch
        if cache is None:
            return None
        if isinstance(cache, dict):
            return {k: self._clone_cache(v) for k, v in cache.items()}
        elif isinstance(cache, torch.Tensor):
            return cache.clone()
        elif isinstance(cache, (list, tuple)):
            return type(cache)(self._clone_cache(v) for v in cache)
        else:
            return cache
    
    def process(self, tokens: list, last_chunk: bool, output_path: str):
        """å¤„ç† tokens å¹¶ç”Ÿæˆ WAV æ–‡ä»¶"""
        if not self.initialized:
            return {"status": "error", "message": "Token2Wav not initialized"}
        
        if self.stream_cache is None:
            return {"status": "error", "message": "Reference audio not set"}
        
        try:
            # ğŸ”§ ä¸´æ—¶é‡å®šå‘ stdout åˆ° stderr
            import sys
            original_stdout = sys.stdout
            sys.stdout = sys.stderr
            
            try:
                import torch
                
                start_time = time.time()
                
                # è®¾ç½®å½“å‰ç¼“å­˜åˆ° token2wav å®ä¾‹
                self.token2wav.stream_cache = self.stream_cache
                self.token2wav.hift_cache_dict = self.hift_cache
                
                # è°ƒç”¨æµå¼ç”Ÿæˆ
                wav_data = self.token2wav.stream(
                    generated_speech_tokens=tokens,
                    prompt_wav=self.ref_audio_path,
                    last_chunk=last_chunk,
                    return_waveform=True
                )
                
                # æ›´æ–°ç¼“å­˜
                self.stream_cache = self.token2wav.stream_cache
                self.hift_cache = self.token2wav.hift_cache_dict
                
                inference_time = time.time() - start_time
                
                # ä¿å­˜ WAV æ–‡ä»¶
                if wav_data is not None and len(wav_data) > 0:
                    # wav_data æ˜¯ numpy arrayï¼Œshape: [1, samples] æˆ– [samples]
                    if len(wav_data.shape) > 1:
                        wav_data = wav_data.squeeze()
                    
                    # å†™å…¥ WAV æ–‡ä»¶
                    sample_rate = 24000
                    audio_duration = len(wav_data) / sample_rate
                    
                    self._write_wav(output_path, wav_data, sample_rate)
                    
                    log(f"ç”Ÿæˆ WAV: {output_path} | {audio_duration:.2f}s | {inference_time*1000:.1f}ms | RTF={inference_time/audio_duration:.2f}")
                    
                    result = {
                        "status": "ok",
                        "message": "WAV generated",
                        "output_path": output_path,
                        "audio_duration": audio_duration,
                        "inference_time_ms": inference_time * 1000,
                        "sample_rate": sample_rate,
                        "num_samples": len(wav_data)
                    }
                else:
                    result = {"status": "ok", "message": "No audio generated", "output_path": None}
            finally:
                # æ¢å¤åŸå§‹ stdout
                sys.stdout = original_stdout
            
            return result
                
        except Exception as e:
            log(f"å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc(file=sys.stderr)
            return {"status": "error", "message": str(e)}
    
    def _write_wav(self, path: str, wav_data: np.ndarray, sample_rate: int):
        """å†™å…¥ WAV æ–‡ä»¶"""
        import struct
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # è½¬æ¢ä¸º 16-bit PCM
        wav_data = np.clip(wav_data, -1.0, 1.0)
        pcm_data = (wav_data * 32767.0).astype(np.int16)
        
        # å†™å…¥ WAV æ–‡ä»¶
        num_channels = 1
        bits_per_sample = 16
        byte_rate = sample_rate * num_channels * (bits_per_sample // 8)
        block_align = num_channels * (bits_per_sample // 8)
        data_size = len(pcm_data) * (bits_per_sample // 8)
        
        with open(path, 'wb') as f:
            # RIFF header
            f.write(b'RIFF')
            f.write(struct.pack('<I', 36 + data_size))
            f.write(b'WAVE')
            
            # fmt chunk
            f.write(b'fmt ')
            f.write(struct.pack('<I', 16))  # chunk size
            f.write(struct.pack('<H', 1))   # audio format (PCM)
            f.write(struct.pack('<H', num_channels))
            f.write(struct.pack('<I', sample_rate))
            f.write(struct.pack('<I', byte_rate))
            f.write(struct.pack('<H', block_align))
            f.write(struct.pack('<H', bits_per_sample))
            
            # data chunk
            f.write(b'data')
            f.write(struct.pack('<I', data_size))
            f.write(pcm_data.tobytes())
    
    def reset(self):
        """é‡ç½®æµå¼ç¼“å­˜åˆ°åˆå§‹çŠ¶æ€"""
        if not self.initialized:
            return {"status": "error", "message": "Token2Wav not initialized"}
        
        try:
            if self.stream_cache_base is not None:
                self.stream_cache = self._clone_cache(self.stream_cache_base)
                self.hift_cache = self._clone_cache(self.hift_cache_base)
                log("æµå¼ç¼“å­˜å·²é‡ç½®")
                return {"status": "ok", "message": "Cache reset"}
            else:
                return {"status": "error", "message": "No base cache to reset from"}
        except Exception as e:
            log(f"é‡ç½®å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}


def main():
    """ä¸»å¾ªç¯ï¼šä» stdin è¯»å–å‘½ä»¤ï¼Œå¤„ç†åå†™å…¥ stdout"""
    log("Token2Wav æœåŠ¡å¯åŠ¨")
    
    service = Token2WavService()
    
    # å‘é€å°±ç»ªä¿¡å·
    print(json.dumps({"status": "ready", "message": "Token2Wav service ready"}), flush=True)
    
    while True:
        try:
            # è¯»å–ä¸€è¡Œ JSON å‘½ä»¤
            line = sys.stdin.readline()
            if not line:
                log("stdin å…³é—­ï¼Œé€€å‡º")
                break
            
            line = line.strip()
            if not line:
                continue
            
            # è§£æå‘½ä»¤
            try:
                cmd = json.loads(line)
            except json.JSONDecodeError as e:
                response = {"status": "error", "message": f"Invalid JSON: {e}"}
                print(json.dumps(response), flush=True)
                continue
            
            cmd_type = cmd.get("cmd", "")
            
            # å¤„ç†å‘½ä»¤
            if cmd_type == "init":
                response = service.init(
                    model_dir=cmd.get("model_dir", ""),
                    device=cmd.get("device", "cuda:0"),
                    float16=cmd.get("float16", True),
                    n_timesteps=cmd.get("n_timesteps", 5)
                )
            elif cmd_type == "set_ref_audio":
                response = service.set_ref_audio(cmd.get("ref_audio_path", ""))
            elif cmd_type == "process":
                response = service.process(
                    tokens=cmd.get("tokens", []),
                    last_chunk=cmd.get("last_chunk", False),
                    output_path=cmd.get("output_path", "")
                )
            elif cmd_type == "reset":
                response = service.reset()
            elif cmd_type == "quit":
                log("æ”¶åˆ°é€€å‡ºå‘½ä»¤")
                response = {"status": "ok", "message": "Goodbye"}
                print(json.dumps(response), flush=True)
                break
            else:
                response = {"status": "error", "message": f"Unknown command: {cmd_type}"}
            
            # å‘é€å“åº”
            print(json.dumps(response), flush=True)
            
        except Exception as e:
            log(f"ä¸»å¾ªç¯å¼‚å¸¸: {e}")
            traceback.print_exc(file=sys.stderr)
            response = {"status": "error", "message": str(e)}
            print(json.dumps(response), flush=True)
    
    log("Token2Wav æœåŠ¡é€€å‡º")


if __name__ == "__main__":
    main()
